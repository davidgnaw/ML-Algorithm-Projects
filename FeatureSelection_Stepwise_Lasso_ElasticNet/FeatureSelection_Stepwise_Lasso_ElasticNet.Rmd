---
title: "HW7 - Q11.1"
author: "Yu Fung David Wang"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
---

**Read in Data**

```{r}
# Goal is to Predict " Crime" 
crime_data <- read.table("uscrime.txt", header = TRUE)
head(crime_data)
```

**Split Data into Train and Test**
```{r}
set.seed("1234")
sample <- sample(c(TRUE, FALSE), nrow(crime_data), replace = TRUE, prob = c(0.7, 0.3))
train <- crime_data[sample,]
test <- crime_data[!sample,]
```


**11.1 (1) Stepwise Regression**

```{r}
# Will Build a Both Direction Stepwise Regression Model

# define intercept-only model
intercept <- lm(Crime ~ 1, data = train)
# define model with predictors
step_model <- lm(Crime~., data = train)
# perform forward stepwise reg
step_forward <- step(intercept, direction = "both", scope = formula(step_model), trace = 0) # trace = 0 to prevent R from displaying full stepwise selection results (too much space)

step_forward$coefficients
step_forward$anova

# Predict
step_predictions <- predict(step_forward, test)

# RMSE
RMSE_step <- sqrt(mean(as.matrix((test$Crime - step_predictions)^2)))
paste("RMSE of Stepwise Regression Model:", RMSE_step)


```
Based on the given coefficients of the model, the model will look like 
- Crime ~ 115.02xPo1 + 67.65xIneq + 196.47xEd + (-3801.84)xProb +105.02x M + 89.37xU2

Based on the ANOVA test, we can see that the akaike info criterion (AIC) is lowest with predictors (Po1, Ineq, Ed, M, Prob, U2). As a lower AIC indicates a better fit model, it shows that all predictors used will produce the most reduction in AIC. All other features were deemed not important.  

The trained model gives a RMSE value of 215.017 on the test data.

**Set Predictor, Response Values (Train) and Test Predictor Values for Lasso and Elastic** 

```{r}
library(glmnet) # in glmnet, there is a standardize function to scale the data

response <- train$Crime 

# predictors have to be in R's matrix format rather than data frame format
predictors <- data.matrix(train[,c('M', 'So', 'Ed', 'Po1', 'Po2', 'LF', 'M.F', 'Pop', 'NW', 'U1', 'U2', 'Wealth', 'Ineq', 'Prob', 'Time')])

test_predictors <- data.matrix(test[,c('M', 'So', 'Ed', 'Po1', 'Po2', 'LF', 'M.F', 'Pop', 'NW', 'U1', 'U2', 'Wealth', 'Ineq', 'Prob', 'Time')])
                          
```


**11.1 (2) Lasso**
```{r}

# First, perform k-fold CV to identify lambda for lowest RMSE
cv_lasso <- cv.glmnet(predictors, response , alpha = 1) # alpha = 1 is the lasso penalty (alpha = 0 is ridge penalty)
opt_lambda <- cv_lasso$lambda.min
paste("Optimal lambda is:", opt_lambda)

# Make sure to scale (standardize) in glmnet 
lasso_model <- glmnet(predictors, response, lambda = opt_lambda, standardize = TRUE, family="gaussian")
coef(lasso_model)

# Predict
lasso_prediction <- predict(lasso_model, s = opt_lambda, newx = test_predictors)

# RMSE
RMSE_lasso <- sqrt(mean(as.matrix((test$Crime - lasso_prediction)^2)))
paste("RMSE of Lasso Regression Model:", RMSE_lasso)

```
Based on the given coefficients of the model, the model will look like
- Crime ~ -3582.998 + 37.12xM + 51.36xSo + 109.44xEd + 102.19xPo1 + 4.99xPo2 + 9.56xM.F + 0.95xPop + 0.96xNW + 20.89xU2 + 50.33xIneq + (-3035.22)xProb

Based on the coefficients, LF, U1, and Wealth were not utilized/not important features. 


**11.1 (3) Elastic Net**

```{r}
# Have to combine ridge and lasso regression

# Have to tune hyperparameter alpha (run cv.glmnet function over wide range of alphas)

elastic_models <- list()

for (i in 0:29) { # looking through 30 different alphas from 0 to 1
  curr_alpha <- paste("alpha", round(i/29, 2))
  elastic_models[[curr_alpha]] <- cv.glmnet(predictors, response, alpha = i/29, standardize = TRUE, family = "gaussian")
}

# Look for best alpha based on evaluation of rmse on test data

elastic_table <- data.frame()
for (i in 0:29) {
  curr_alpha <- paste("alpha", round(i/29, 2))
  curr_elastic_model <- elastic_models[[curr_alpha]]
  
  # predictions
  elastic_prediction <- predict(curr_elastic_model, curr_elastic_model$lambda.1se, newx = test_predictors)
  
  # RMSE
RMSE_elastic <- sqrt(mean(as.matrix((test$Crime - elastic_prediction)^2)))

# Store into elastic_table results
  elastic_table <- rbind(elastic_table, data.frame(Alpha = round(i/29, 2), 2, RMSE = RMSE_elastic))
  
}

elastic_table

min_RMSE <- min(elastic_table$RMSE)
min_alpha <- elastic_table$Alpha[elastic_table$RMSE == min_RMSE]

paste("The min RMSE of ", min_RMSE, "is found with a alpha value of", min_alpha)

# Use the alpha that provides min RMSE to look at coefficients
table_alpha_name <- paste("alpha", min_alpha)
predict(elastic_models[[table_alpha_name]], type ="coef")

```

The coefficients are seen above, as the results/regression formula constantly change, will not write it into equation form. However, generally, feautres LF, U1, U2, Wealth, and Time are not utilized for the regression based on Elastic Net Regression. 


